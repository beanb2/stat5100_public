\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{float}

% Set the margins on the page to not be so large
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

% Take off page numbering
\pagenumbering{gobble}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{%
  7.1.1 Generalized Additive Models \\
  \smallskip
  \large Stat 5100: Dr. Bean
}
\date{}

\maketitle

\section*{Example 1: Baseball Dataset from 4.1.1}

Let's see if we can improve upon the penalized linear regression model to predict the log of salary for professional (non-pitcher) baseball players. Note that answers will differ slightly depending on the random seed set.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Set a random seed for reproducibility}
\hlkwd{set.seed}\hlstd{(}\hlnum{830578}\hlstd{)}

\hlcom{# Load data}
\hlkwd{library}\hlstd{(stat5100)}
\hlkwd{library}\hlstd{(mgcv)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: nlme}}

{\ttfamily\noindent\itshape\color{messagecolor}{\#\# This is mgcv 1.8-33. For overview type 'help("{}mgcv-package"{})'.}}\begin{alltt}
\hlkwd{data}\hlstd{(baseball)}

\hlstd{baseball_gam_all} \hlkwb{<-}
  \hlstd{mgcv}\hlopt{::}\hlkwd{gam}\hlstd{(logSalary} \hlopt{~} \hlkwd{s}\hlstd{(nAtBat)} \hlopt{+} \hlkwd{s}\hlstd{(nHits)} \hlopt{+} \hlkwd{s}\hlstd{(nHome)} \hlopt{+}
             \hlkwd{s}\hlstd{(nRuns)} \hlopt{+} \hlkwd{s}\hlstd{(nRBI)} \hlopt{+} \hlkwd{s}\hlstd{(nBB)} \hlopt{+} \hlkwd{s}\hlstd{(YrMajor)} \hlopt{+}
             \hlkwd{s}\hlstd{(CrAtBat)} \hlopt{+} \hlkwd{s}\hlstd{(CrHits)} \hlopt{+} \hlkwd{s}\hlstd{(CrHome)} \hlopt{+} \hlkwd{s}\hlstd{(CrRuns)} \hlopt{+}
             \hlkwd{s}\hlstd{(CrRbi)} \hlopt{+} \hlkwd{s}\hlstd{(CrBB)} \hlopt{+} \hlkwd{s}\hlstd{(nOuts)} \hlopt{+} \hlkwd{s}\hlstd{(nAssts)} \hlopt{+}
             \hlkwd{s}\hlstd{(nError)} \hlopt{+} \hlstd{League} \hlopt{+} \hlstd{Division,}
                         \hlkwc{data} \hlstd{= baseball)}

\hlkwd{summary}\hlstd{(baseball_gam_all)}
\end{alltt}
\begin{verbatim}
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## logSalary ~ s(nAtBat) + s(nHits) + s(nHome) + s(nRuns) + s(nRBI) + 
##     s(nBB) + s(YrMajor) + s(CrAtBat) + s(CrHits) + s(CrHome) + 
##     s(CrRuns) + s(CrRbi) + s(CrBB) + s(nOuts) + s(nAssts) + s(nError) + 
##     League + Division
## 
## Parametric coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)     5.90256    0.03572 165.223   <2e-16 ***
## LeagueNational  0.07437    0.04414   1.685   0.0936 .  
## DivisionWest   -0.02042    0.04346  -0.470   0.6389    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##              edf Ref.df      F p-value    
## s(nAtBat)  3.470  4.373  1.312 0.25485    
## s(nHits)   1.000  1.000  0.130 0.71892    
## s(nHome)   4.819  5.884  1.259 0.25922    
## s(nRuns)   4.174  5.178  1.223 0.29111    
## s(nRBI)    1.971  2.551  0.975 0.42890    
## s(nBB)     1.245  1.445  3.885 0.04937 *  
## s(YrMajor) 5.315  6.270 10.713 < 2e-16 ***
## s(CrAtBat) 7.385  8.063  2.822 0.00894 ** 
## s(CrHits)  3.895  4.930  2.157 0.05747 .  
## s(CrHome)  8.094  8.510  3.217 0.00128 ** 
## s(CrRuns)  1.000  1.000  0.079 0.77944    
## s(CrRbi)   8.447  8.770  2.961 0.00632 ** 
## s(CrBB)    1.000  1.000  0.138 0.71063    
## s(nOuts)   5.957  7.070  1.982 0.05850 .  
## s(nAssts)  1.000  1.000  0.325 0.56924    
## s(nError)  3.601  4.463  1.224 0.25678    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.882   Deviance explained = 91.1%
## GCV = 0.12437  Scale est. = 0.093457  n = 263
\end{verbatim}
\end{kframe}
\end{knitrout}

Now, let's refit the models but only using the significant terms:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{baseball_gam} \hlkwb{<-} \hlstd{mgcv}\hlopt{::}\hlkwd{gam}\hlstd{(logSalary} \hlopt{~} \hlkwd{s}\hlstd{(nBB)} \hlopt{+} \hlkwd{s}\hlstd{(YrMajor)} \hlopt{+} \hlkwd{s}\hlstd{(CrAtBat)} \hlopt{+} \hlkwd{s}\hlstd{(CrHits)} \hlopt{+}
                           \hlkwd{s}\hlstd{(CrHome)} \hlopt{+} \hlkwd{s}\hlstd{(CrRbi)} \hlopt{+} \hlkwd{s}\hlstd{(nOuts),} \hlkwc{data} \hlstd{= baseball)}
\end{alltt}
\end{kframe}
\end{knitrout}

We can take a look at the estimated spline functions for each of the predictor variables. In each of the below plots, the $x$-axis contains the various levels of the predictor variables. On the $y$-axis, we see the estimated spline function (keep in mind that these are multiple different polynomial functions being concatenated together). Along the $x$-axis you will see little notches: these each indicate the unique points that went into creating the spline.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mgcv}\hlopt{::}\hlkwd{plot.gam}\hlstd{(baseball_gam)}
\end{alltt}
\end{kframe}
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-1} 
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-2} 
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-3} 
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-4} 
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-5} 
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-6} 
\includegraphics[width=0.50\textwidth]{figure/unnamed-chunk-3-7} 

\end{knitrout}

For simplicity, if we fit a GAM with just CrAtBat and nBB (like we did in the LOESS example), then we get the following surface plot:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{gam_baseball_demo_mgcv.png}
	\label{fig:myfig}
\end{figure}

This plot is comparable to the plot from the LOESS example in 4.3.1.

\bigskip
\hrule
\bigskip

\section*{Example 2: Diabetes Dataset}

The Pima Indians Diabetes dataset is a dataset from the National Institute of Diabetes and Digestive and Kidney Diseases. Our goal here is to predict whether or not a patient has diabetes. In this dataset, all patients are females that are at least 21 and are of Pima Indian heritage.

Let's split our data into a training and testing dataset and see how well we do on the testing dataset by training on the training dataset.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(}\hlstr{"diabetes"}\hlstd{)}
\hlkwd{head}\hlstd{(diabetes)}
\end{alltt}
\begin{verbatim}
##   Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI
## 1           6     148            72            35       0 33.6
## 2           1      85            66            29       0 26.6
## 3           8     183            64             0       0 23.3
## 4           1      89            66            23      94 28.1
## 5           0     137            40            35     168 43.1
## 6           5     116            74             0       0 25.6
##   DiabetesPedigreeFunction Age Outcome
## 1                    0.627  50       1
## 2                    0.351  31       0
## 3                    0.672  32       1
## 4                    0.167  21       0
## 5                    2.288  33       1
## 6                    0.201  30       0
\end{verbatim}
\begin{alltt}
\hlcom{# How many observations are there?}
\hlkwd{nrow}\hlstd{(diabetes)}
\end{alltt}
\begin{verbatim}
## [1] 768
\end{verbatim}
\begin{alltt}
\hlcom{# Create a training and testing split with 80% training data}
\hlstd{train_index} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(diabetes),} \hlkwc{size} \hlstd{=} \hlnum{0.80}\hlopt{*}\hlkwd{nrow}\hlstd{(diabetes))}
\hlstd{diabetes_train} \hlkwb{<-} \hlstd{diabetes[train_index, ]}
\hlstd{diabetes_test} \hlkwb{<-} \hlstd{diabetes[}\hlopt{-}\hlstd{train_index, ]}

\hlstd{diabetes_gam} \hlkwb{<-} \hlstd{mgcv}\hlopt{::}\hlkwd{gam}\hlstd{(Outcome} \hlopt{~} \hlkwd{s}\hlstd{(Pregnancies)} \hlopt{+} \hlkwd{s}\hlstd{(Glucose)} \hlopt{+} \hlkwd{s}\hlstd{(BloodPressure)} \hlopt{+}
                           \hlkwd{s}\hlstd{(SkinThickness)} \hlopt{+} \hlkwd{s}\hlstd{(Insulin)} \hlopt{+} \hlkwd{s}\hlstd{(BMI)} \hlopt{+}
                           \hlkwd{s}\hlstd{(DiabetesPedigreeFunction)} \hlopt{+} \hlkwd{s}\hlstd{(Age),} \hlkwc{family} \hlstd{=} \hlstr{"binomial"}\hlstd{,}
                         \hlkwc{data} \hlstd{= diabetes_train)}
\end{alltt}
\end{kframe}
\end{knitrout}

Now let's see how accurate we are on the testing dataset:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Here are the predicted class probabilities}
\hlstd{test_class_prob} \hlkwb{<-} \hlkwd{predict}\hlstd{(diabetes_gam, diabetes_test,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)}

\hlcom{# If the probability is higher than 50% of having diabetes, mark it as a 1.}
\hlstd{pred_class} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{nrow}\hlstd{(diabetes_test))}
\hlstd{pred_class[test_class_prob} \hlopt{>} \hlnum{0.50}\hlstd{]} \hlkwb{<-} \hlnum{1}

\hlcom{# Now that we have our predicted class, let's get some statistics on our accuracy.}
\hlstd{total_test} \hlkwb{<-} \hlkwd{nrow}\hlstd{(diabetes_test)}
\hlstd{total_correct} \hlkwb{<-} \hlkwd{sum}\hlstd{(pred_class} \hlopt{==} \hlstd{diabetes_test}\hlopt{$}\hlstd{Outcome)}

\hlcom{# Error rate}
\hlstd{(total_test} \hlopt{-} \hlstd{total_correct)} \hlopt{/} \hlstd{total_test}
\end{alltt}
\begin{verbatim}
## [1] 0.1948052
\end{verbatim}
\begin{alltt}
\hlcom{# Successful prediction rate}
\hlstd{total_correct} \hlopt{/} \hlstd{total_test}
\end{alltt}
\begin{verbatim}
## [1] 0.8051948
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{document}
