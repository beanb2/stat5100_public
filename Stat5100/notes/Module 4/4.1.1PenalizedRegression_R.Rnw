\documentclass{article}

\usepackage{float}
\usepackage{amsmath}

% Set the margins on the page to not be so large
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

% Take off page numbering
\pagenumbering{gobble}

\begin{document}

\title{%
4.1.1: R - Penalized Regression Methods \\
(Ridge Regression, LASSO, and Elastic Net) \\
\smallskip
\large Stat 5100: Dr. Bean
}
\date{}

\maketitle

\subsubsection*{Matrix Specification}

Up to this point in the course, we have specified models using formula notation such as: 

<<eval=FALSE>>=
linear_model <- lm(response ~ x1 + x2, data = mydata)
@

However, there are some functions in R that require a matrix specification. This requires us to organize the explanatory variables as a matrix ($X$) (performing variable transformations beforehand) and the response variable as a vector ($y$). As an example:

<<eval=FALSE>>=
# Either:
model <- somemodel(y, X)
# Or in some packages:
model <- somemodel(X, y)
@

In the above, the variable $X$ is a matrix in R where each column would contain our predictor variables $x_1$, $x_2$, etc. and our rows would contain the different observations. For example, $X_{i, j}$ (meaning the $i^\text{th}$ row of $X$ and the $j^\text{th}$ column of $X$) would refer to the recorded value of $x_j$ (the $j^\text{th}$ predictor variable for the the $i^\text{th}$ observation in our data.

The matrix vs formula model specification is a product of open source software: each R package software contributor requires different model inputs in order for their functions to run properly. This in mind, always be sure to study the function documentation before you start trying to use a new R package. While there are formula-based version of the penalized regression techniques, these notes use a package that requires matrix style inputs due to its ease of implementation and consistency across the three main penalized regression approaches. 

\medskip
\hrule
\medskip

\textbf{Example: } (Ridge Regression; recall Handout 2.6.1 example) A study seeks to relate (in females) amount of body fat ($Y$) to triceps skinfold thickness ($X_1$), thigh circumference ($X_2$), and midarm circumference ($X_3$).  Amount of body fat is expensive to measure, requiring immersion of person in water.  This expense motivates the desire for a predictive model based on these inexpensive predictors.

<<out.width = "0.6\\textwidth", fig.align='center'>>=
# Load the data
library(stat5100)
data(bodyfat)

# Look at the original fit along with VIF:
bodyfat_lm <- lm(body ~ triceps + thigh + midarm, data = bodyfat)

summary(bodyfat_lm)

# VIF:
olsrr::ols_vif_tol(bodyfat_lm)

# Try ridge regression as a remedial measure
# -----------------------------------------
# We use the glmnet() function inside the glmnet package to do this. Note that
# instead of specifying our model using a formula (formulas in R are of the
# form Y ~ X1 + X2 + X3), we create a dataframe of just our predictor variables
# and a vector of our response variable.
y <- bodyfat$body

# Our X must come in the form of a matrix. First we take out the "body" column
# from the dataframe, and then we convert it to a matrix.
X <- as.matrix(subset(bodyfat, select = -body))

# Ridge regression requires that we first standarize our explanatory variables. 
# However, the glmnet implementation does this automatically for you 
# within the function so we do not need to standardize prior to use. 

# Rather than select an optimal value of lambda using the trace plot 
# (see 4.1 notes), we will select an optimal value for lambda by minimizing 
# the 5-fold cross validated error. 
set.seed(123) # Set seed for reproducibility. 
bodyfat_test_ridge_lm <- glmnet::cv.glmnet(X, y, alpha = 0, nfolds = 5)
bodyfat_test_ridge_lm
@

Here let's pick $\lambda = 0.437$ based upon the above output.

<<out.width = "0.6\\textwidth", fig.align='center'>>=
# Use the non-cv version to actually create a model that we can use and predict with.
bodyfat_ridge_lm <- glmnet::glmnet(X, y, alpha = 0, lambda = 0.437)
# Look at coefficients
bodyfat_ridge_lm$beta

# Store our coefficients. You could do this by manually entering the numbers,
# but we index them here for better automation.
triceps_coef <- bodyfat_ridge_lm$beta[1]
thigh_coef <- bodyfat_ridge_lm$beta[2]
midarm_coef <- bodyfat_ridge_lm$beta[3]
@

In order to get $b_0$ for the \textit{unstandardized} coefficients, we use the formula:
\[\beta_0 = \bar{Y} - \beta_1 \bar{X}_1 - \beta_2 \bar{X}_2 - \beta_3 \bar{X}_3\]

<<>>=
# Means of various variables
mean(bodyfat$body)
mean(bodyfat$triceps)
mean(bodyfat$thigh)
mean(bodyfat$midarm)

# Crunch our b0 formula:
b0_estimate <- mean(bodyfat$body) - (triceps_coef * mean(bodyfat$triceps)) -
  (thigh_coef * mean(bodyfat$thigh)) - (midarm_coef * mean(bodyfat$midarm))
b0_estimate

# Confirm that our "by hand" approach matches the intercept calculated 
# automatically by glmnet.  
bodyfat_ridge_lm$a0
@

\subsubsection*{Get predicted values in ridge regression}

<<out.width = "0.6\\textwidth", fig.align='center'>>=
predicted_y <- predict(bodyfat_ridge_lm, X)

# Plot the predicted values vs observed
plot(y, predicted_y, xlab = "Observed Body Fat", ylab = "Predicted Body Fat",
     main = "Predicted Y vs. Observed Y in Ridge Regression")
@

\medskip
\hrule
\medskip

\subsection*{Example 2: Baseball}

This data set (from SAS Help: the dataset has been imported into this R package) contains salary (for 1987) and performance (1986 and some career) data for 322 MLB players who played at least one game in both 1986 and 1987 seasons, excluding pitchers. How can salary be predicted from performance?

<<>>=
# Load and take a look at the baseball dataset
data(baseball)
head(baseball)

# Subset the dataset to retain only variables that are relevant for prediction
# and remove all NAs prior to model input (algorithm fails if NAs are retained)
baseball_sub <- subset(baseball, select = c(logSalary, nAtBat, nHits, 
                                            nHome, nRuns, nRBI, nBB, YrMajor, 
                                            CrAtBat, CrHits, CrHome, CrRuns, 
                                            CrRbi, CrBB, nOuts, nAssts, nError, 
                                            League, Division))

baseball_sub <- na.omit(baseball_sub)

# Take log-transformation of salary and create design matrix.
# Note that this function only retains observations with no missing values 
# for the variables specified in the formula. We removed missing values 
# prior to this step in order to keep X (baseball_design) and y
# (baseball_sub$logSalary) aligned. 
# The [, -1] omits the intercept column since glm will fit one automatically
baseball_design <- model.matrix(object = logSalary ~ nAtBat + nHits + nHome + 
                                  nRuns + nRBI + nBB + YrMajor + CrAtBat + 
                                  CrHits + CrHome + CrRuns + CrRbi + CrBB + 
                                  nOuts + nAssts + nError + League + Division,
                                data = baseball_sub)[, -1] 
@

First, let's use Lasso regression:

<<>>=
baseball_lasso_optimal <- glmnet::cv.glmnet(baseball_design, 
                                            baseball_sub$logSalary, 
                                            alpha = 1)
baseball_lasso_optimal$lambda.min

# Pick optimal lambda from the above
baseball_lasso <- glmnet::glmnet(baseball_design, 
                                 baseball_sub$logSalary,
                                 alpha = 1, 
                                 lambda = baseball_lasso_optimal$lambda.min)
baseball_lasso$beta
baseball_lasso$a0
@

Now, let's show an example with elastic net regression. This is specified by using some alpha between 0 and 1. For simplicity, we we will pick $\alpha = 0.5$.

<<>>=
baseball_elnet_optimal <- glmnet::cv.glmnet(baseball_design, 
                                            baseball_sub$logSalary, 
                                            alpha = 0.5)
baseball_elnet_optimal$lambda.min

# Pick optimal lambda from the above
baseball_elnet <- glmnet::glmnet(baseball_design, 
                                 baseball_sub$logSalary,
                                 alpha = 0.5, 
                                 lambda = baseball_elnet_optimal$lambda.min)

# Obtain beta estimates (including intercept)
baseball_elnet$beta
baseball_elnet$a0
@


\end{document}
